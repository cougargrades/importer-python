#!/usr/bin/env python3

import os
import os.path
import sys
import sqlite3
import json
import argparse
from tqdm import tqdm
from halo import Halo

parser = argparse.ArgumentParser(description='Prepare a SQLite database into Firestore-ready JSONL files')
parser.add_argument('dbfile', metavar='records.db', type=str,
                    help='Path to the SQLite database generated by csv2db.py')
parser.add_argument('--out', dest='folder', default=None,
                    help='Folder to store .jsonl files in')

args = parser.parse_args()

# check arguments
if not os.path.isfile(args.dbfile):
    print(f'{args.dbfile} is not a file.')
    exit(1)

if args.folder == None:
    print(f'--out must be a folder name.')
    exit(1)
else:
    # if not a directory and not an existing file
    if not os.path.isdir(args.folder) and not os.path.isfile(args.folder):
        # create the folder
        os.mkdir(args.folder)
    if(not os.path.isdir(os.path.join(args.folder, 'catalog')) and not os.path.isfile(os.path.join(args.folder, 'catalog'))):
        # create the subfolder
        os.mkdir(os.path.join(args.folder, 'catalog'))
    if(not os.path.isdir(os.path.join(args.folder, 'catalog.meta')) and not os.path.isfile(os.path.join(args.folder, 'catalog.meta'))):
        # create the subfolder
        os.mkdir(os.path.join(args.folder, 'catalog.meta'))

# https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection.row_factory
def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

# setup sqlite
conn = sqlite3.connect(args.dbfile)
conn.row_factory = dict_factory
c = conn.cursor()
c.execute('SELECT DISTINCT DEPT, CATALOG_NBR FROM records ORDER BY DEPT, CATALOG_NBR;')
unique_courses = c.fetchall()
c.execute('SELECT COUNT(*) FROM records;')
total_rows = c.fetchone()["COUNT(*)"]

print(f'{len(unique_courses)} distinct courses and {total_rows} total rows in {args.dbfile}')

spinner = Halo(text='Writing collection `catalog.meta` ...', spinner='dots')
spinner.start()
c.execute('SELECT * FROM catalog_meta;')
catalog_meta = c.fetchall() # [{'latestTerm': 201901}]
catalog_meta = catalog_meta[0]
with open(os.path.join(args.folder, 'catalog.meta', 'meta.json'), 'w') as f:
    f.write(f'{json.dumps(catalog_meta)}')
spinner.succeed()

print('Writing collection `catalog/` ...')

# assign an outfile file
for row in unique_courses:
    row["outfile"] = f'{row["DEPT"]} {row["CATALOG_NBR"]}.jsonl'

# progress bar
with tqdm(total=total_rows, unit="rows") as t:
    i = 1 # used in the progress bar description to indicate what course is being processed
    # for every unique course ()
    for row in unique_courses:
        t.set_description(f'[{i}/{len(unique_courses)}] {row["outfile"]}')
        # get all sections
        c.execute('SELECT * FROM records WHERE DEPT=? AND CATALOG_NBR=?', (row["DEPT"], row["CATALOG_NBR"]))
        sections = c.fetchall()
        # the first line is a header
        meta = {
            "department": row["DEPT"],
            "catalogNumber": row["CATALOG_NBR"],
            "description": sections[0]["COURSE_DESCR"],
            "cumulativeGPAmin": None,
            "cumulativeGPAmax": None,
            "cumulativeGPA": None,
            "sectionCount": 0 # leave at zero for automatic incrementation in Cloud Functions
        }
        # write the file
        with open(os.path.join(args.folder, 'catalog', row["outfile"]), 'w') as f:
            # write the header line
            f.write(f'{json.dumps(meta)}\n')

            # hold individual sections in memory to be de-duped after the first traversal of `sections`
            cache = []
            # for every section
            for sec in sections:
                # write JSON in the new schema
                data = {
                    "term": sec["TERM_CODE"],
                    "termString": sec["TERM"],
                    "sectionNumber": sec["CLASS_SECTION"],
                    "semesterGPA": sec["AVG_GPA"],
                    "A": sec["A"],
                    "B": sec["B"],
                    "C": sec["C"],
                    "D": sec["D"],
                    "F": sec["F"],
                    "Q": sec["Q"],
                    "instructorNames": []
                }
                # check for multiple instructors teaching this section
                # https://stackoverflow.com/a/25373204
                dups = list(filter(lambda s: s["TERM_CODE"] == sec["TERM_CODE"] and s["CLASS_SECTION"] == sec["CLASS_SECTION"], sections)) # array of complete sqlite dicts
                # lamba function counts number of names in a provided list
                countNames = lambda l, first, last: sum(1 if v["INSTR_FIRST_NAME"] == first and v["INSTR_LAST_NAME"] == last else 0 for v in l)
                # de-dupe dups for instructors that are listed twice for the same section number
                for j in range(len(dups)-1,0,-1):
                    # if this instructor has multiple records for some reason
                    if(countNames(dups, dups[j]["INSTR_FIRST_NAME"], dups[j]["INSTR_LAST_NAME"]) > 1):
                        dups.pop(j)
                # for every instructor that taught this exact section
                for d in dups:
                    # append a new object to "instructors" property
                    data["instructorNames"] += [{
                        "firstName": d["INSTR_FIRST_NAME"],
                        "lastName": d["INSTR_LAST_NAME"],
                        "termGPAmin": d["PROF_MIN"],
                        "termGPAmax": d["PROF_MAX"],
                        "termGPA": d["PROF_AVG"],
                        "termSectionsTaught": d["PROF_COUNT"]
                    }]
                # append to cache
                cache += [data]
            # counts the number of exact section instances in a list
            countSections = lambda l, term, secNum: sum(1 if v["term"] == term and v["sectionNumber"] == secNum else 0 for v in l)
            # for every item in cache (traverse backwards because removing items)
            for j in range(len(cache)-1,0,-1):
                # if this exact section has multiple occurences
                if(countSections(cache, cache[j]["term"], cache[j]["sectionNumber"]) > 1):
                    # remove it
                    cache.pop(j)
                    # update the progress bar for sections that were removed so that it's not 95% when done
                    t.update()
            # for every item in cache
            for p in cache:
                # write to jsonl file
                f.write(f'''{json.dumps(p)}\n''')
                # update progress bar
                t.update()
        # increment the course counter
        i += 1